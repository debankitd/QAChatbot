# ğŸ¤– AI Data Assistant

An intelligent document-aware chatbot built with **Streamlit**, **LangChain**, **FAISS**, and **OpenAI GPT models**.  
Upload your PDFs, Word, Text, or PowerPoint files â€” and chat with your data in natural language.

---

## ğŸ“˜ Technical Documentation

A detailed step-by-step explanation of the applicationâ€™s design, logic, and architecture can be found in the file:  
ğŸ“„ **AI Data Assistant.docx**

---


## ğŸŒŸ Features

- ğŸ“‚ **Multi-format document support** â€” PDF, DOCX, TXT, PPTX  
- ğŸ§  **Context-aware Q&A** using GPT-4 and LangChain RetrievalQA  
- ğŸ” **FAISS-based vector search** for fast and semantic retrieval  
- ğŸ§¾ **Automatic summarization** for large documents  
- ğŸ’¬ **Interactive Streamlit chat interface**  
- ğŸ–¼ï¸ **OCR fallback** for scanned PDFs (via Tesseract)  
- âš™ï¸ **Session memory** for conversational context  

---

## ğŸ—ï¸ Architecture
```
User â†’ Upload Files â†’ Text Extraction â†’ Chunking & Embedding â†’ FAISS Indexing
â†“
Chat Query â†’ Context Retrieval â†’ GPT-4 Response Generation â†’ Display
```
---


## ğŸ§° Tech Stack

| Layer | Technology |
|-------|-------------|
| **Frontend** | Streamlit |
| **LLM Orchestration** | LangChain |
| **Vector Store** | FAISS |
| **Embedding Model** | OpenAI Embeddings |
| **Language Model** | GPT-3.5 / GPT-4 |
| **Document Parsing** | pdfplumber, python-docx, python-pptx |
| **OCR** | pytesseract + Pillow |
| **Environment Management** | python-dotenv |

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/yourusername/ai-data-assistant.git
cd ai-data-assistant

2ï¸âƒ£ Create and activate a virtual environment
python -m venv venv
source venv/bin/activate    # macOS/Linux
venv\Scripts\activate       # Windows

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Add your OpenAI API key
Create a .env file in the project root:
OPENAI_API_KEY=your_openai_api_key_here

5ï¸âƒ£ Run the Streamlit app
streamlit run DataAssistantChatbot.py

Then open the provided local URL in your browser.
```
---

## ğŸ§  How It Works

1. **Text Extraction** â€” Uses format-specific libraries and OCR fallback.  
2. **Chunking** â€” Splits large text into 1000-character overlapping chunks.  
3. **Embedding** â€” Creates semantic embeddings using OpenAI models.  
4. **Indexing** â€” Stores embeddings in FAISS for efficient retrieval.  
5. **Query Processing** â€” Fetches relevant chunks and generates a GPT-based response.

---

## ğŸš€ Future Improvements

- Persistent vector databases (e.g., Pinecone, Chroma)  
- Metadata-based search and filtering  
- Multi-user session management  
- Enhanced image and table parsing  
- Streamlit dashboard for analytics  

---
